{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I shall write down a step by step data cleaning process from all the insights gathered from the previuos notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import customized functions\n",
    "from src.data_cleaning import cleaning_functions as cfs\n",
    "from src.data_cleaning import exploration_functions as efs\n",
    "from src.data_cleaning import processing_functions as pfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Load data and make Train Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_split():\n",
    "    \"\"\"In this function we use the Pandas read_csv method to \n",
    "    convert the csv files into dataframes. These csv files are \n",
    "    saved in hte data folder as:\n",
    "    >>> 'training_set_values.csv' (for X features)\n",
    "    >>> 'training_set_labels.csv' (for y target)\n",
    "    \n",
    "    We then split the dataframes into training and testing sets \n",
    "    and return the X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    #read csvs\n",
    "    import pandas as pd\n",
    "    features = pd.read_csv('../../data/training_set_values.csv')\n",
    "    labels = pd.read_csv('../../data/training_set_labels.csv')\n",
    "    \n",
    "    #train and test split, random_state of 2020, test_size = 25%\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=2020, test_size=0.25) \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim down X_train and X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identified columns that we will be dropping:\n",
    "\n",
    "`id`, `date_recorded`, `recorded_by`, `wpt_name`, `scheme_name`, `num_private`, `subvillage`,\n",
    "`ward`, `longitude`, `latitude`, `extraction_type_class`, `management_group`, `payment_type`, \n",
    "`quality_group`, `quantity_group`, `source_type`, `source_class`, `waterpoint_type_group`, `installer`, `funder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_feature_columns(df):\n",
    "    \"\"\"This function drops all the investigated unnecessary \n",
    "    columns from the features dataframe and returns the \n",
    "    trimmed datadrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    df.drop(['id', 'date_recorded', 'recorded_by', 'wpt_name',\n",
    "             'scheme_name', 'num_private', 'subvillage', 'ward',\n",
    "             'longitude', 'latitude', 'extraction_type_class', \n",
    "             'management_group', 'payment_type', 'quality_group',\n",
    "             'quantity_group', 'source_type', 'source_class', \n",
    "             'waterpoint_type_group', 'installer', 'funder'], \n",
    "              axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train = drop_unnecessary_feature_columns(X_train)\n",
    "X_test = drop_unnecessary_feature_columns(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs.fill_all_nans(X_train)\n",
    "cfs.fill_all_nans(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert y_train and y_test `status_group` to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first drop id\n",
    "def drop_id_from_y(df):\n",
    "    \"\"\"This function drops id column \n",
    "    from the y sets\n",
    "    \"\"\"\n",
    "    df.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#convert `status_group` to classes using LabelEncoder\n",
    "def create_class_labels_for_y(df1, df2):\n",
    "    \"\"\"This function takes the y training set as df1\n",
    "    and y test set as df2 and creates labels \n",
    "    for the target.\n",
    "    Here we fit on training and transform both the \n",
    "    training and the test set\n",
    "    Returns: training df, test df and label dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(df1['status_group'])\n",
    "    target1 = le.transform(df1['status_group'])\n",
    "    df1 = pd.DataFrame(target1, columns=['target'])\n",
    "    \n",
    "    target2 = le.transform(df2['status_group'])\n",
    "    df2 = pd.DataFrame(target2, columns=['target'])\n",
    "    \n",
    "    \n",
    "    classes_dict = {k:v for k,v in zip(\n",
    "                            le.transform(['functional', 'functional needs repair', 'non functional']), \n",
    "                            ['functional', 'functional needs repair', 'non functional'])}\n",
    "    \n",
    "    return df1, df2, classes_dict\n",
    "\n",
    "def process_y_sets(df1, df2):\n",
    "    \"\"\"This function preprocess the y_train and y_test\n",
    "    by dropping the id column and converting the status_group\n",
    "    to classes \n",
    "    \n",
    "    Returns: y_train, y_test and label dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    #helper functio to drop id\n",
    "    df1 = drop_id_from_y(df1)\n",
    "    df2 = drop_id_from_y(df2)\n",
    "    \n",
    "    #helper function to make class labels\n",
    "    y_train, y_test, classes_dict = create_class_labels_for_y(df1, df2)\n",
    "    \n",
    "    return y_train, y_test, classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, classes_dict = process_y_sets(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       2\n",
       "4       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'functional', 1: 'functional needs repair', 2: 'non functional'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining all functions here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_dataset():\n",
    "    \"\"\"This function loads the two datasets from the data folder\n",
    "    and splits it up into X_train, X_test, y_train, y_test \n",
    "    with random_state = 2020, test_size = 25%\n",
    "    \n",
    "    Then it trims down X Features and create y labels\n",
    "    \n",
    "    Returns: X_train, X_test, y_train, y_test, classes_dict\n",
    "    \"\"\"\n",
    "    #using helper function to split dataset\n",
    "    X_train, X_test, y_train, y_test = load_data_and_split()\n",
    "    \n",
    "    #using helper function to trim down X features\n",
    "    X_train = drop_unnecessary_feature_columns(X_train)\n",
    "    X_test = drop_unnecessary_feature_columns(X_test)\n",
    "    \n",
    "    #using helper functio to fill NaNs\n",
    "    fill_all_nans(X_train)\n",
    "    fill_all_nans(X_test)\n",
    "    \n",
    "    #using helper function to process y_train and y_test\n",
    "    y_train, y_test, classes_dict = process_y_sets(y_train, y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, classes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, classes_dict = pfs.processed_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'functional', 1: 'functional needs repair', 2: 'non functional'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amount_tsh               float64\n",
       "gps_height                 int64\n",
       "basin                     object\n",
       "region                    object\n",
       "region_code                int64\n",
       "district_code              int64\n",
       "lga                       object\n",
       "population                 int64\n",
       "public_meeting            object\n",
       "scheme_management         object\n",
       "permit                    object\n",
       "construction_year          int64\n",
       "extraction_type           object\n",
       "extraction_type_group     object\n",
       "management                object\n",
       "payment                   object\n",
       "water_quality             object\n",
       "quantity                  object\n",
       "source                    object\n",
       "waterpoint_type           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifying_feature_types(df):\n",
    "    \"\"\"This function lists out the names of columns\n",
    "    in a dataframe whose dtypes are objects as categorical\n",
    "    features and the numeric types as numeric features\n",
    "    \n",
    "    Returns two lists of columns names first for numeric \n",
    "    and the second for categorical features\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "    for name, dtype in zip(df.dtypes.index, df.dtypes):\n",
    "        if str(dtype).startswith('ob'):\n",
    "            categorical_features.append(name)\n",
    "        else:\n",
    "            numeric_features.append(name)\n",
    "    \n",
    "    return numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats, cat_feats = identifying_feature_types(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount_tsh',\n",
       " 'gps_height',\n",
       " 'region_code',\n",
       " 'district_code',\n",
       " 'population',\n",
       " 'construction_year']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basin',\n",
       " 'region',\n",
       " 'lga',\n",
       " 'public_meeting',\n",
       " 'scheme_management',\n",
       " 'permit',\n",
       " 'extraction_type',\n",
       " 'extraction_type_group',\n",
       " 'management',\n",
       " 'payment',\n",
       " 'water_quality',\n",
       " 'quantity',\n",
       " 'source',\n",
       " 'waterpoint_type']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_feature(df, name):\n",
    "    \"\"\"This funciton takes in a dataframe and a feature name and \n",
    "    One hot encodes the feature and adds it to the dataframe\n",
    "    \n",
    "    Returns transformed dataframe and the ohe object \n",
    "    used to transform the frame\n",
    "    \"\"\"\n",
    "    \n",
    "    ohe = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    single_feature_df = df[[name]]\n",
    "    ohe.fit(single_feature_df)\n",
    "    feature_array = ohe.transform(single_feature_df).toarray()\n",
    "    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0], index=df.index)\n",
    "    df = df.drop(name, axis=1)\n",
    "    df = pd.concat([df, ohe_df], axis=1)\n",
    "    \n",
    "    #returning ohe here so that it can be used to transform X_test later\n",
    "    return df, ohe\n",
    "\n",
    "#can use this function in a loop to convert all the categorical features into encoded features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's encode all the categorical features and add them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_all_categorical_features(df):\n",
    "    \"\"\"This function takes in a dataframe, identifies the\n",
    "    dtypes in the dataframe and uses the object dtypes to\n",
    "    list out categorical columns\n",
    "    \n",
    "    Next it use OneHotEncoder to convert those Categorical \n",
    "    features\n",
    "    \n",
    "    Returns: the transformed dataframe and a dictionary \n",
    "    containing the ohe object that can be used later to \n",
    "    transform the testing dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #helper function to identify categorical feature names\n",
    "    num_feats, cat_feats = identifying_feature_types(df)\n",
    "    \n",
    "    #reassuring the values in categorical features are str types\n",
    "    for name in cat_feats:\n",
    "        df[name] = df[name].astype(str)\n",
    "    \n",
    "    #use helper function in loop to transform dataframe\n",
    "    encoders = {}\n",
    "    \n",
    "    for name in cat_feats:\n",
    "        df, ohe = one_hot_encode_feature(df, name)\n",
    "        encoders[name] = ohe\n",
    "    \n",
    "    return df, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to see if this worked...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, encoders = ohe_all_categorical_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>Internal</th>\n",
       "      <th>Lake Nyasa</th>\n",
       "      <th>Lake Rukwa</th>\n",
       "      <th>Lake Tanganyika</th>\n",
       "      <th>...</th>\n",
       "      <th>shallow well</th>\n",
       "      <th>spring</th>\n",
       "      <th>unknown</th>\n",
       "      <th>cattle trough</th>\n",
       "      <th>communal standpipe</th>\n",
       "      <th>communal standpipe multiple</th>\n",
       "      <th>dam</th>\n",
       "      <th>hand pump</th>\n",
       "      <th>improved spring</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29193</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35859</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1228</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>650</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57006</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15959</th>\n",
       "      <td>0.0</td>\n",
       "      <td>954</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18523</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1944</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>1984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44739</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1521</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47734</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40259</th>\n",
       "      <td>500.0</td>\n",
       "      <td>1293</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41824</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44550 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       amount_tsh  gps_height  region_code  district_code  population  \\\n",
       "29193         0.0           0           19              8           0   \n",
       "35859      1200.0        1228           16              2         650   \n",
       "57006         0.0           0           17              5           0   \n",
       "15959         0.0         954           21              4         200   \n",
       "21582         0.0           0           12              4           0   \n",
       "...           ...         ...          ...            ...         ...   \n",
       "18523         0.0        1944           15              3         150   \n",
       "44739         0.0        1521           15              2         100   \n",
       "47734         0.0           0           12              6           0   \n",
       "40259       500.0        1293           10              3          60   \n",
       "41824         0.0           0           12              7           0   \n",
       "\n",
       "       construction_year  Internal  Lake Nyasa  Lake Rukwa  Lake Tanganyika  \\\n",
       "29193                  0       0.0         0.0         0.0              0.0   \n",
       "35859               2002       0.0         0.0         0.0              1.0   \n",
       "57006                  0       0.0         0.0         0.0              1.0   \n",
       "15959               2003       0.0         0.0         0.0              0.0   \n",
       "21582                  0       0.0         1.0         0.0              0.0   \n",
       "...                  ...       ...         ...         ...              ...   \n",
       "18523               1984       0.0         0.0         1.0              0.0   \n",
       "44739               1991       0.0         0.0         1.0              0.0   \n",
       "47734                  0       0.0         0.0         1.0              0.0   \n",
       "40259               2002       0.0         1.0         0.0              0.0   \n",
       "41824                  0       0.0         0.0         0.0              0.0   \n",
       "\n",
       "       ...  shallow well  spring  unknown  cattle trough  communal standpipe  \\\n",
       "29193  ...           1.0     0.0      0.0            0.0                 0.0   \n",
       "35859  ...           0.0     0.0      0.0            0.0                 1.0   \n",
       "57006  ...           1.0     0.0      0.0            0.0                 0.0   \n",
       "15959  ...           1.0     0.0      0.0            0.0                 0.0   \n",
       "21582  ...           0.0     1.0      0.0            0.0                 1.0   \n",
       "...    ...           ...     ...      ...            ...                 ...   \n",
       "18523  ...           0.0     0.0      0.0            0.0                 1.0   \n",
       "44739  ...           1.0     0.0      0.0            0.0                 0.0   \n",
       "47734  ...           0.0     1.0      0.0            0.0                 0.0   \n",
       "40259  ...           0.0     1.0      0.0            0.0                 1.0   \n",
       "41824  ...           0.0     0.0      0.0            0.0                 1.0   \n",
       "\n",
       "       communal standpipe multiple  dam  hand pump  improved spring  other  \n",
       "29193                          0.0  0.0        1.0              0.0    0.0  \n",
       "35859                          0.0  0.0        0.0              0.0    0.0  \n",
       "57006                          0.0  0.0        1.0              0.0    0.0  \n",
       "15959                          0.0  0.0        0.0              0.0    1.0  \n",
       "21582                          0.0  0.0        0.0              0.0    0.0  \n",
       "...                            ...  ...        ...              ...    ...  \n",
       "18523                          0.0  0.0        0.0              0.0    0.0  \n",
       "44739                          0.0  0.0        0.0              0.0    1.0  \n",
       "47734                          0.0  0.0        0.0              0.0    1.0  \n",
       "40259                          0.0  0.0        0.0              0.0    0.0  \n",
       "41824                          0.0  0.0        0.0              0.0    0.0  \n",
       "\n",
       "[44550 rows x 259 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That worked!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "* Start with the modeling\n",
    "* Finalize the metric/metrics that are important to answer our business question\n",
    "* Look up ensemble methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Tanzania)",
   "language": "python",
   "name": "tanzania"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
